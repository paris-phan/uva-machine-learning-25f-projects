{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAQal0Tqm85E",
        "outputId": "bffd7985-213a-4758-c933-7529f923de1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX59fFntkoNy",
        "outputId": "9c650d4e-e9d5-4184-e9a4-a4e81e73e55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'nasa-cmapss-2-engine-degradation' dataset.\n",
            "Path to dataset files: /kaggle/input/nasa-cmapss-2-engine-degradation\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "import gc\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bishals098/nasa-cmapss-2-engine-degradation\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogDnqp9I6BQo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt53bBcAAAwM",
        "outputId": "f3761c25-ce1a-422e-f750-4548468d8c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using HDF5 file: /kaggle/input/nasa-cmapss-2-engine-degradation/N-CMAPSS_DS01-005.h5\n",
            "<bound method NDFrame.head of 0          1\n",
            "1000       1\n",
            "2000       1\n",
            "3000       1\n",
            "4000       1\n",
            "          ..\n",
            "4906631    0\n",
            "4906632    0\n",
            "4906633    0\n",
            "4906634    0\n",
            "4906635    0\n",
            "Name: target, Length: 348983, dtype: int64>\n",
            "(348983,)\n",
            "no unit columns: Index(['cycle', 'Fc', 'hs', 'HPT_eff_mod', 'alt', 'Mach', 'TRA', 'T2', 'T24',\n",
            "       'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50',\n",
            "       'Nf', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21', 'W22', 'W25', 'W31',\n",
            "       'W32', 'W48', 'W50', 'SmFan', 'SmLPC', 'SmHPC', 'phi'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming `path` is already defined from kagglehub.dataset_download(...)\n",
        "h5_path = path + \"/N-CMAPSS_DS01-005.h5\"\n",
        "print(\"Using HDF5 file:\", h5_path)\n",
        "\n",
        "# 1. Load TRAIN/DEV arrays and column names from HDF5\n",
        "with h5py.File(h5_path, 'r') as hdf:\n",
        "    # Main dev arrays (TRAIN DATA)\n",
        "    A_dev  = hdf[\"A_dev\"][:]      # (N, 4)  -> ['unit', 'cycle', 'Fc', 'hs']\n",
        "    T_dev  = hdf[\"T_dev\"][:]      # (N, 10) -> health modifiers\n",
        "    W_dev  = hdf[\"W_dev\"][:]      # (N, 4)  -> ['alt', 'Mach', 'TRA', 'T2']\n",
        "    Xs_dev = hdf[\"X_s_dev\"][:]    # (N, 14) -> physical sensors\n",
        "    Xv_dev = hdf[\"X_v_dev\"][:]    # (N, 14) -> virtual sensors\n",
        "    Y_dev  = hdf[\"Y_dev\"][:]      # (N, 1)  -> RUL target\n",
        "\n",
        "    # Column names (decode from bytes -> str)\n",
        "    A_var  = [x.decode(\"utf-8\") for x in hdf[\"A_var\"][:]]\n",
        "    T_var  = [x.decode(\"utf-8\") for x in hdf[\"T_var\"][:]]\n",
        "    W_var  = [x.decode(\"utf-8\") for x in hdf[\"W_var\"][:]]\n",
        "    Xs_var = [x.decode(\"utf-8\") for x in hdf[\"X_s_var\"][:]]\n",
        "    Xv_var = [x.decode(\"utf-8\") for x in hdf[\"X_v_var\"][:]]\n",
        "gc.collect()\n",
        "# 2. Stack all TRAIN feature blocks horizontally into one big matrix\n",
        "X_all_train = np.hstack([A_dev, T_dev, W_dev, Xs_dev, Xv_dev])\n",
        "\n",
        "# 3. Build full feature name list in the same order\n",
        "feature_names = A_var + T_var + W_var + Xs_var + Xv_var\n",
        "\n",
        "# 4. Create full TRAIN pandas DataFrame (includes unit, cycle, all features)\n",
        "df_dev = pd.DataFrame(X_all_train, columns=feature_names)\n",
        "\n",
        "# 5. Add RUL label as a new column\n",
        "df_dev[\"RUL\"] = Y_dev[:, 0]\n",
        "\n",
        "# 6. Remove constant (useless) columns\n",
        "constant_cols = [\n",
        "    'fan_eff_mod', 'fan_flow_mod',\n",
        "    'LPC_eff_mod', 'LPC_flow_mod',\n",
        "    'HPC_eff_mod', 'HPC_flow_mod',\n",
        "    'HPT_flow_mod', 'LPT_eff_mod', 'LPT_flow_mod'\n",
        "]\n",
        "\n",
        "# Pandas TRAIN dataset\n",
        "df_dev_clean = df_dev.drop(columns=constant_cols)\n",
        "\n",
        "# forward fill data\n",
        "df_dev_clean = df_dev_clean.ffill()\n",
        "\n",
        "# 7. Define feature columns (drop RUL + unit)\n",
        "#feature_cols = [c for c in df_dev_clean.columns if c not in [\"RUL\", \"unit\"]]\n",
        "#feature_cols = [c for c in df_dev_clean.columns if c not in [\"RUL\"]]\n",
        "\n",
        "# ---- PANDAS TRAIN SET ----\n",
        "#X_train_df = df_dev_clean[feature_cols]   # features (no unit, includes cycle)\n",
        "X_train_df = df_dev_clean\n",
        "df_dev_clean['target'] = np.where(df_dev_clean[\"RUL\"]>5, 1, 0)\n",
        "y_train_df = df_dev_clean[\"target\"]          # labels (RUL>5 --> 1; o/w, 0)\n",
        "\n",
        "# take every 1000th row unless a failure row\n",
        "failure_mask = (y_train_df == 0)\n",
        "row_mask = (y_train_df.reset_index().index % 1000 == 0)\n",
        "y_train_df = y_train_df[failure_mask | row_mask]\n",
        "X_train_df = X_train_df[failure_mask | row_mask]\n",
        "\n",
        "print(y_train_df.head)\n",
        "print(y_train_df.shape)\n",
        "#values_to_keep = np.where(condition_mask)[0]\n",
        "#values_to_keep = np.where(y_test_np == 0)[0]\n",
        "#X_train_df = X_train_df.iloc[::1000]\n",
        "#y_train_df = y_train_df.iloc[::1000]\n",
        "\n",
        "# ---- NUMPY TRAIN SET ----\n",
        "no_unit_pd_x = X_train_df.drop(labels = ['unit', 'RUL', 'target'], axis=1)\n",
        "print(f\"no unit columns: {no_unit_pd_x.columns}\")\n",
        "# some feature selection\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = RFC()\n",
        "model.fit(no_unit_pd_x, y_train_df)\n",
        "rf_importances = pd.Series(model.feature_importances_, index=no_unit_pd_x.columns)\n",
        "n_features = 10\n",
        "(rf_importances.nlargest(n_features).plot(kind='barh', title = 'Top Feature Importances RF'))\n",
        "plt.show()\n",
        "\n",
        "no_unit_pd_x_rf = no_unit_pd_x[rf_importances.nlargest(n_features).index[:10]]\n",
        "no_unit_pd_x_svm = no_unit_pd_x[rf_importances.nlargest(n_features).index[:5]]\n",
        "print(f\"after feature selection: {no_unit_pd_x}\")\n",
        "X_train_np_rf = no_unit_pd_x_rf.to_numpy()\n",
        "X_train_np_svm = no_unit_pd_x_svm.to_numpy()\n",
        "y_train_np = y_train_df.to_numpy()\n",
        "\n",
        "X_train_df = X_train_df.drop(columns = ['RUL', 'target'])\n",
        "print(f\"column labels: {X_train_df.columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bundi4hSc7Z"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Make sure h5_path is already defined as in your train code\n",
        "print(\"Using HDF5 file:\", h5_path)\n",
        "\n",
        "# 1. Load TEST arrays and column names from HDF5\n",
        "with h5py.File(h5_path, 'r') as hdf:\n",
        "    # Main TEST arrays\n",
        "    A_test = hdf[\"A_test\"][:]      # (N_test, 4)  -> ['unit', 'cycle', 'Fc', 'hs']\n",
        "    T_test = hdf[\"T_test\"][:]      # (N_test, 10) -> health modifiers\n",
        "    W_test = hdf[\"W_test\"][:]      # (N_test, 4)  -> ['alt', 'Mach', 'TRA', 'T2']\n",
        "    Xs_test = hdf[\"X_s_test\"][:]   # (N_test, 14) -> physical sensors\n",
        "    Xv_test = hdf[\"X_v_test\"][:]   # (N_test, 14) -> virtual sensors\n",
        "    Y_test = hdf[\"Y_test\"][:]      # (N_test, 1)  -> RUL target\n",
        "\n",
        "    # Column names (same as train)\n",
        "    A_var  = [x.decode(\"utf-8\") for x in hdf[\"A_var\"][:]]\n",
        "    T_var  = [x.decode(\"utf-8\") for x in hdf[\"T_var\"][:]]\n",
        "    W_var  = [x.decode(\"utf-8\") for x in hdf[\"W_var\"][:]]\n",
        "    Xs_var = [x.decode(\"utf-8\") for x in hdf[\"X_s_var\"][:]]\n",
        "    Xv_var = [x.decode(\"utf-8\") for x in hdf[\"X_v_var\"][:]]\n",
        "gc.collect()\n",
        "# 2. Stack all TEST feature blocks horizontally into one big matrix\n",
        "X_test_all = np.hstack([A_test, T_test, W_test, Xs_test, Xv_test])\n",
        "\n",
        "# 3. Build full feature name list in the same order (same as train)\n",
        "feature_names = A_var + T_var + W_var + Xs_var + Xv_var\n",
        "\n",
        "# 4. Create full TEST pandas DataFrame (includes unit, cycle, all features)\n",
        "df_test = pd.DataFrame(X_test_all, columns=feature_names)\n",
        "\n",
        "# 5. Add RUL label as a new column for TEST\n",
        "df_test[\"RUL\"] = Y_test[:, 0]\n",
        "\n",
        "# 6. Remove constant (useless) columns in TEST (same list you used for train)\n",
        "constant_cols = [\n",
        "    'fan_eff_mod', 'fan_flow_mod',\n",
        "    'LPC_eff_mod', 'LPC_flow_mod',\n",
        "    'HPC_eff_mod', 'HPC_flow_mod',\n",
        "    'HPT_flow_mod', 'LPT_eff_mod', 'LPT_flow_mod'\n",
        "]\n",
        "\n",
        "df_test_clean = df_test.drop(columns=constant_cols)\n",
        "\n",
        "# 7. Define feature columns (same logic: drop RUL + unit)\n",
        "#feature_cols_test = [c for c in df_test_clean.columns if c not in [\"RUL\", \"unit\"]]\n",
        "#feature_cols_test = [c for c in df_test_clean.columns if c not in [\"RUL\"]]\n",
        "\n",
        "# ---- PANDAS TEST SET ----\n",
        "#X_test_df = df_test_clean[feature_cols_test]   # pandas features (no unit, includes cycle)\n",
        "X_test_df = df_test_clean\n",
        "df_test_clean['target'] = np.where(df_test_clean[\"RUL\"]>1, 1, 0)\n",
        "y_test_df = df_test_clean[\"target\"]              # pandas labels\n",
        "\n",
        "# only take every 1000 row or where failure occurs\n",
        "failure_mask = (y_test_df == 0)\n",
        "row_mask = (y_test_df.reset_index().index % 1000 == 0)\n",
        "y_test_df = y_test_df[failure_mask | row_mask]\n",
        "X_test_df = X_test_df[failure_mask | row_mask]\n",
        "\n",
        "\n",
        "# ---- NUMPY TEST SET ----\n",
        "no_unit_pd_x = X_test_df.drop(labels = ['unit', 'RUL', 'target'], axis=1)\n",
        "no_unit_pd_x_rf = no_unit_pd_x[rf_importances.nlargest(n_features).index[:10]]\n",
        "no_unit_pd_x_svm = no_unit_pd_x[rf_importances.nlargest(n_features).index[:5]]\n",
        "print(f\"after feature selection: {no_unit_pd_x}\")\n",
        "X_test_np_rf = no_unit_pd_x_rf.to_numpy()\n",
        "X_test_np_svm = no_unit_pd_x_svm.to_numpy()\n",
        "y_test_np = y_test_df.to_numpy()\n",
        "#values_to_keep = np.where(y_test_np == 0)[0]\n",
        "#print(y_test_np)\n",
        "#print(values_to_keep)\n",
        "#print(values_to_keep.shape)\n",
        "#indices = range(0, int(len(y_test_np)/10))\n",
        "#indices = np.array(indices)\n",
        "#indices = indices*10\n",
        "#print(indices.shape)\n",
        "#values_to_keep = np.concatenate((values_to_keep, indices))\n",
        "#X_test_np = X_test_np[values_to_keep]\n",
        "#y_test_np = y_test_np[values_to_keep]\n",
        "\n",
        "print(\"\\n=== TEST DATA SHAPES ===\")\n",
        "print(\"df_test_clean:\", df_test_clean.shape)\n",
        "print(f\"df labels: {X_test_df.columns}\")\n",
        "print(f\"df for np: {no_unit_pd_x.columns}\")\n",
        "print(\"X_test_df:\", X_test_df.shape, \"| y_test_df:\", y_test_df.shape)\n",
        "print(\"X_test_np_rf:\", X_test_np_rf.shape, \"| y_test_np:\", y_test_np.shape)\n",
        "print(\"X_test_np_svm:\", X_test_np_svm.shape, \"| y_test_np:\", y_test_np.shape)\n",
        "print(\"Test units:\", df_test_clean['unit'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZcfZhfuqn3B"
      },
      "outputs": [],
      "source": [
        "del hdf\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5CUfJNlUnDS"
      },
      "outputs": [],
      "source": [
        "## General helper methods\n",
        "def fold(x, y, i, nfolds):\n",
        "    # your code\n",
        "    print(f\"fold {i}\")\n",
        "    x_len = len(y)\n",
        "    val_split_start = int((i)*(x_len/nfolds))\n",
        "    val_split_end = int((i+1)*(x_len/nfolds))\n",
        "    if val_split_end>=x_len:\n",
        "      x_val = x[val_split_start:]\n",
        "      y_val = y[val_split_start:]\n",
        "      x_train = x[:val_split_start]\n",
        "      y_train = y[:val_split_start]\n",
        "    elif val_split_start==0:\n",
        "      x_val = x[:val_split_end]\n",
        "      y_val = y[:val_split_end]\n",
        "      x_train = x[val_split_end:]\n",
        "      y_train = y[val_split_end:]\n",
        "    else:\n",
        "      x_val = x[val_split_start:val_split_end]\n",
        "      y_val = y[val_split_start:val_split_end]\n",
        "      x_train = np.append(x[:val_split_start], x[val_split_end:], axis=0)\n",
        "      y_train = np.append(y[:val_split_start], y[val_split_end:], axis=0)\n",
        "    return x_train, y_train, x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6Dg3PZXVCGR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkwXqkXwT7Bl"
      },
      "outputs": [],
      "source": [
        "from scipy import stats as st\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "import tensorflow as tf\n",
        "def single_hyper_test_approach(x_train, y_train, hyperparameters, model_type):\n",
        "  #shuffle training data\n",
        "  train_indices = [idx for idx in range(len(y_train))]\n",
        "  import random\n",
        "  random.seed(0)\n",
        "  random.shuffle(train_indices)\n",
        "  count = 0\n",
        "  x_train = x_train[train_indices]\n",
        "  y_train = y_train[train_indices]\n",
        "  nfolds = 4\n",
        "  acc_by_fold = np.zeros(nfolds)\n",
        "  #cross validation\n",
        "  for num_fold in range(nfolds):\n",
        "    fold_x_train, fold_y_train, fold_x_val, fold_y_val = fold(x_train, y_train, num_fold, nfolds)\n",
        "\n",
        "    if model_type == \"SVM\":\n",
        "      #fitted_model = SVM_fitting(torch.from_numpy(fold_x_train).to(device), torch.from_numpy(fold_y_train).to(device), hyperparameters)\n",
        "      fitted_model = SVM_fitting(fold_x_train, fold_y_train, hyperparameters)\n",
        "    else:\n",
        "      #fitted_model = tree_fitting(torch.from_numpy(fold_x_train).to(device), torch.from_numpy(fold_y_train).to(device), hyperparameters)\n",
        "      fitted_model = tree_fitting(fold_x_train, fold_y_train, hyperparameters)\n",
        "\n",
        "    y_pred = y_predict(fitted_model, fold_x_val)\n",
        "    fold_acc = calc_accuracy(y_pred, fold_y_val)\n",
        "    acc_by_fold[num_fold] = fold_acc\n",
        "  accuracy=0\n",
        "  for acc in acc_by_fold:\n",
        "    accuracy+=acc\n",
        "  accuracy = accuracy/nfolds\n",
        "  return accuracy\n",
        "def SVM_fitting(x_train, y_train, hyperparameters):\n",
        "  model = SVC(kernel=hyperparameters['kernel'], C=hyperparameters['C'], degree=hyperparameters['degree'])\n",
        "  model.fit(np.asarray(x_train), np.asarray(y_train))\n",
        "  return model\n",
        "def tree_fitting(x_train, y_train, hyperparameters):\n",
        "  model = RFC(max_depth=hyperparameters['max_depth'], min_samples_split = hyperparameters['min_sample_splits'], n_estimators=hyperparameters['n_estimators'])\n",
        "  model.fit(np.asarray(x_train), np.asarray(y_train))\n",
        "  return model\n",
        "def y_predict(fitted_model, test_data):\n",
        "  return fitted_model.predict(test_data)\n",
        "def calc_accuracy(y_predict, y):\n",
        "    # your code\n",
        "    num_accurate = 0\n",
        "    for y_loc in range(len(y_predict)):\n",
        "      if y_predict[y_loc] == y[y_loc]:\n",
        "        num_accurate += 1\n",
        "    acc = num_accurate/len(y_predict)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgxV-aYUpncS"
      },
      "outputs": [],
      "source": [
        "# these are features importances\n",
        "print(rf_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nOQOZp-oDhE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OvUmLcPVWH5V"
      },
      "outputs": [],
      "source": [
        "# ! pip install --upgrade pytorch-forecasting\n",
        "# ! pip install lightning\n",
        "# ! pip install pytorch-lightning\n",
        "# ! pip install torchrl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xU1b5F-hzar1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import torch\n",
        "# from pytorch_forecasting.models.temporal_fusion_transformer._tft import TemporalFusionTransformer\n",
        "# from pytorch_forecasting.data.timeseries._timeseries import TimeSeriesDataSet\n",
        "# from torch.nn import CrossEntropyLoss\n",
        "# from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "# from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "# from torchrl.record.loggers.csv import CSVLogger\n",
        "# from lightning.pytorch import Trainer\n",
        "# ## FORWARD FILL\n",
        "\n",
        "# def tft(x_train, y_train, x_test):\n",
        "#   print(x_train)\n",
        "#   x_train.apply(pd.to_numeric)\n",
        "#   y_train.apply(pd.to_numeric)\n",
        "#   x_test.apply(pd.to_numeric)\n",
        "#   x_train['cycle'] = x_train['cycle'].astype(int)\n",
        "#   x_test['cycle'] = x_test['cycle'].astype(int)\n",
        "#   print(f\"checking columns : {x_train.columns}\")\n",
        "#   print(f\"checking rows: {x_train.index.unique()}\")\n",
        "#   training = TimeSeriesDataSet(\n",
        "#       x_train,\n",
        "#       time_idx=\"cycle\",\n",
        "#       target=\"RUL\",\n",
        "#       group_ids=['unit'],\n",
        "\n",
        "#       min_encoder_length=50, # Make sure each sequence has atleast 50 sequences\n",
        "#       max_encoder_length=50,\n",
        "#       min_prediction_length=1,\n",
        "#       max_prediction_length=1,\n",
        "\n",
        "#       static_categoricals=[],\n",
        "#       static_reals=[\"unit\"],\n",
        "#       time_varying_known_categoricals=[],\n",
        "#       time_varying_known_reals=[ \"cycle\",],\n",
        "#       time_varying_unknown_reals=[ \"Fc\", \"hs\", \"alt\", \"Mach\", \"TRA\", \"T2\",\"T24\", \"T30\", \"T48\", \"T50\", \"P15\", \"P2\", \"P21\", \"P24\",\"Ps30\", \"P40\", \"P50\", \"Nf\", \"Nc\", \"Wf\",\"T40\", \"P30\", \"P45\", \"W21\", \"W22\", \"W25\", \"W31\", \"W32\",\"W48\", \"W50\", \"SmFan\", \"SmLPC\", \"SmHPC\", \"phi\",],\n",
        "\n",
        "#       # target_normalizer=NaNLabelEncoder(add_nan=True),\n",
        "#       #categorical_encoders=categorical_encoders,\n",
        "#       add_relative_time_idx=True,\n",
        "#       add_target_scales=False,\n",
        "#       add_encoder_length=True,\n",
        "#       allow_missing_timesteps=True\n",
        "#   )\n",
        "#   train_dataloader = training.to_dataloader(\n",
        "#     train=True, batch_size=120, num_workers=3)\n",
        "\n",
        "#   validation = TimeSeriesDataSet.from_dataset(\n",
        "#       training, x_test, predict=True, stop_randomization=True)\n",
        "\n",
        "#   val_dataloader = validation.to_dataloader(\n",
        "#       train=False, batch_size=120 * 10, num_workers=3)\n",
        "#   tft = TemporalFusionTransformer.from_dataset(\n",
        "#       training,\n",
        "#       learning_rate=0.01,\n",
        "#       hidden_size=128,\n",
        "#       lstm_layers=2, # avoid increasing\n",
        "#       attention_head_size=8,\n",
        "#       dropout=0.1,\n",
        "#       hidden_continuous_size=32,\n",
        "#       loss=CrossEntropyLoss(),  # Key difference for classification\n",
        "#       output_size=2,\n",
        "#       optimizer='ranger', # try 'adamW'\n",
        "#       share_single_variable_networks=True,\n",
        "#       causal_attention=False,\n",
        "#   )\n",
        "#   early_stop_callback = EarlyStopping(\n",
        "#     monitor=\"val_loss\", min_delta=1e-4, patience=16, verbose=True, mode=\"min\"\n",
        "#   )\n",
        "\n",
        "#   checkpoint_callback = ModelCheckpoint(\n",
        "#       monitor=\"val_loss\",\n",
        "#       save_top_k=5,\n",
        "#       every_n_epochs=1,\n",
        "#       dirpath=\"checkpoints/\",\n",
        "#       filename=\"tft-epoch-{epoch:02d}-val_loss-{val_loss:.4f}\",\n",
        "#   )\n",
        "\n",
        "#   csv_logger = CSVLogger(\"logs/\", name=\"tft\")\n",
        "#   trainer = Trainer(\n",
        "#     max_epochs=400, # ranger optimizer is slow\n",
        "#     accelerator=\"auto\",\n",
        "#     devices=\"auto\",\n",
        "#     enable_model_summary=True,\n",
        "#     gradient_clip_val=0.1,\n",
        "#     callbacks=[checkpoint_callback, early_stop_callback],\n",
        "#     logger=csv_logger,\n",
        "#     log_every_n_steps=1,\n",
        "#     check_val_every_n_epoch=1,\n",
        "#     enable_checkpointing=True\n",
        "#   )\n",
        "\n",
        "#   trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
        "#   pred = tft.predict(validation)\n",
        "#   #label_idx = int(pred[0][0][1:].argmax() + 1)\n",
        "\n",
        "#   # Convert to gesture name\n",
        "#   # gesture = reverse_map[label_idx]\n",
        "\n",
        "#   # # Store in results list\n",
        "#   # results.append({\"sequence_id\": seq, \"gesture\": gesture})\n",
        "#   # inverse_map = {v: k for k, v in categorical_encoders['gesture']._mapping.items()}\n",
        "#   # predicted_label = inverse_map[predicted_class_id]\n",
        "\n",
        "#   return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ocmoo2k2vnh"
      },
      "outputs": [],
      "source": [
        "def hypertuning_svm_rf(x_train_rf, x_train_svm, y_train):\n",
        "  best_svm_acc = 0\n",
        "  best_rf_acc = 0\n",
        "  best_svm_hypers = {}\n",
        "  best_rf_hypers = {}\n",
        "  kernel = ['rbf'] #, 'linear', 'poly']\n",
        "  C = [1] # , 2]\n",
        "  degree = [2] #, 3]\n",
        "  for kernel_ind in kernel:\n",
        "    for c_ind in C:\n",
        "      if kernel_ind == 'poly':\n",
        "        for degree_ind in degree:\n",
        "          hyperparameters = {\n",
        "          'kernel': kernel_ind,\n",
        "          'C': c_ind,\n",
        "          'degree': degree\n",
        "          }\n",
        "          accuracy = single_hyper_test_approach(x_train_svm, y_train, hyperparameters, \"SVM\")\n",
        "          print(\"SVM:\")\n",
        "          print(f\"{hyperparameters}, accuracy: {accuracy}\")\n",
        "          if best_svm_acc < accuracy:\n",
        "            best_svm_acc = accuracy\n",
        "            best_svm_hypers = hyperparameters\n",
        "      else:\n",
        "        hyperparameters = {\n",
        "        'kernel': kernel_ind,\n",
        "        'C': c_ind,\n",
        "        'degree': 1\n",
        "        }\n",
        "        accuracy = single_hyper_test_approach(x_train_svm, y_train, hyperparameters, \"SVM\")\n",
        "        print(\"SVM:\")\n",
        "        print(f\"{hyperparameters}, accuracy: {accuracy}\")\n",
        "        if best_svm_acc < accuracy:\n",
        "          best_svm_acc = accuracy\n",
        "          best_svm_hypers = hyperparameters\n",
        "\n",
        "  n_estimators = [50, 100]\n",
        "  min_sample_splits = [2, 3]\n",
        "  max_depth = [2, 4]\n",
        "  for n_est in n_estimators:\n",
        "    for min_split in min_sample_splits:\n",
        "      for depth in max_depth:\n",
        "        hyperparameters = {\n",
        "          'n_estimators': n_est,\n",
        "          'min_sample_splits': min_split,\n",
        "          'max_depth': depth\n",
        "        }\n",
        "        accuracy = single_hyper_test_approach(x_train_rf, y_train, hyperparameters, \"RF\")\n",
        "        print(\"RF:\")\n",
        "        print(f\"n_estimators: {n_est}, min_sample_splits: {min_split}, max_depth: {depth}, accuracy: {accuracy}\")\n",
        "        if best_rf_acc < accuracy:\n",
        "          best_rf_acc = accuracy\n",
        "          best_rf_hypers = hyperparameters\n",
        "  return best_rf_hypers, best_svm_hypers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVPjryTNFD5L"
      },
      "outputs": [],
      "source": [
        "x_train_rf = X_train_np_rf\n",
        "x_train_svm = X_train_np_svm\n",
        "y_train = y_train_np\n",
        "x_test_rf = X_test_np_rf\n",
        "x_test_svm = X_test_np_svm\n",
        "y_test = y_test_np\n",
        "\n",
        "best_rf_hypers, best_svm_hypers = hypertuning_svm_rf(x_train_rf, x_train_svm, y_train)\n",
        "\n",
        "fitted_rf = tree_fitting(x_train_rf, y_train, best_rf_hypers)\n",
        "rf_prediction = y_predict(fitted_rf, x_test_rf)\n",
        "rf_accuracy = calc_accuracy(rf_prediction, y_test)\n",
        "print(f\"rf overall accuracy: {rf_accuracy}\")\n",
        "\n",
        "fitted_svm = SVM_fitting(x_train_svm, y_train, best_svm_hypers)\n",
        "svm_prediction = y_predict(fitted_svm, x_test_svm)\n",
        "svm_accuracy = calc_accuracy(svm_prediction, y_test)\n",
        "print(f\"svm overall accuracy: {svm_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34FqkCEP62ss"
      },
      "outputs": [],
      "source": [
        "rf_train_pred = y_predict(fitted_rf, x_train_rf)\n",
        "rf_train_accuracy = calc_accuracy(rf_train_pred, y_train)\n",
        "\n",
        "svm_train_pred = y_predict(fitted_svm, x_train_svm)\n",
        "svm_train_accuracy = calc_accuracy(svm_train_pred, y_train)\n",
        "\n",
        "models = [\"RF\", \"SVM\"]\n",
        "train_accs = [rf_train_accuracy, svm_train_accuracy]\n",
        "test_accs = [rf_accuracy, svm_accuracy]\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(x - width/2, train_accs, width, label='Training')\n",
        "plt.bar(x + width/2, test_accs, width, label='Testing')\n",
        "plt.xticks(x, models)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Testing Accuracy')\n",
        "plt.legend()\n",
        "plt.tightlayout()\n",
        "plt.show()\n",
        "\n",
        "# RF confusion matrix\n",
        "rf_cm = confusion_matrix(y_test, rf_prediction)\n",
        "disp_rf = ConfusionMatrixDisplay(confusion_matrix=rf_cm, display_labels=[0, 1])\n",
        "plt.figure(figsize=(4,4))\n",
        "disp_rf.plot(values_format=\"d\")\n",
        "plt.title(\"Random Forest - Test Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# SVM confusion matrix\n",
        "svm_cm = confusion_matrix(y_test, svm_prediction)\n",
        "disp_svm = ConfusionMatrixDisplay(confusion_matrix=svm_cm, display_labels=[0, 1])\n",
        "plt.figure(figsize=(4,4))\n",
        "disp_svm.plot(values_format=\"d\")\n",
        "plt.title(\"SVM - Test Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OppJCYyUa6eL"
      },
      "outputs": [],
      "source": [
        "# x_train = X_train_df\n",
        "# y_train = y_train_df\n",
        "# x_test = df_test_clean\n",
        "# tft_prediction = tft(x_train, y_train, x_test)\n",
        "# tft_accuracy = calc_accuracy(tft_prediction, y_test)\n",
        "# print(f\"tft overall accuracy: {tft_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlEQOuRobF_9"
      },
      "outputs": [],
      "source": [
        "# svm output\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "SVM:\n",
        "{'kernel': 'rbf', 'C': 1, 'degree': 1}, accuracy: 0.996842253175913\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "SVM:\n",
        "{'kernel': 'rbf', 'C': 2, 'degree': 1}, accuracy: 0.9970313740866987\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7dEoySgqkG5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP57eH5lqkV3"
      },
      "outputs": [],
      "source": [
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 2, max_depth: 2, accuracy: 0.9982635261409849\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 2, max_depth: 4, accuracy: 0.9988194265371614\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 2, max_depth: 8, accuracy: 0.9999426907166977\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 3, max_depth: 2, accuracy: 0.9980572129575725\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 3, max_depth: 4, accuracy: 0.9988194265371614\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 3, max_depth: 8, accuracy: 0.9999340943012118\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 4, max_depth: 2, accuracy: 0.998358086350049\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 4, max_depth: 4, accuracy: 0.9988194265371614\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 50, min_sample_splits: 4, max_depth: 8, accuracy: 0.9999169015030835\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 2, max_depth: 2, accuracy: 0.9983151028603336\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 2, max_depth: 4, accuracy: 0.9988194265371614\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 2, max_depth: 8, accuracy: 0.9999484216384593\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 3, max_depth: 2, accuracy: 0.9979225362961782\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 3, max_depth: 4, accuracy: 0.9988194265371614\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 3, max_depth: 8, accuracy: 0.9999541525602207\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 4, max_depth: 2, accuracy: 0.9978480343132794\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 4, max_depth: 4, accuracy: 0.9988194265371614\n",
        "fold 0\n",
        "fold 1\n",
        "fold 2\n",
        "fold 3\n",
        "RF:\n",
        "n_estimators: 100, min_sample_splits: 4, max_depth: 8, accuracy: 0.99995128709934"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}