{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PMA9MlHZ4AU"
   },
   "source": [
    "Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Google Colab\n",
    "!pip install diffusers transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aOvJTJbb95c"
   },
   "source": [
    "Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gM6p8klc9vF"
   },
   "source": [
    "Image Generator *Unlimited Image Generations + Pretty Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Huggin Face provided a pipeline to interact and use the image generator (in this case I am using Segmind Stable Diffusion 1B (SSD-1B) which is free and fast! Though not as great in queality as today's models.)\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"segmind/SSD-1B\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLQZ7NXzeHEC"
   },
   "source": [
    "###Approach 1: Basic Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyNUnUXVYIgD"
   },
   "source": [
    "Custom Environment for Human-In-The-Loop Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import M\n",
    "from IPython.display import display\n",
    "#Following OpenGym's custom environment setup interface (though not using it for opengym)\n",
    "\n",
    "class ArtCreationEnv():\n",
    "  #begin with basic initialization\n",
    "  def __init__(self, image_generator, obs_space, model = 'q-learn'):\n",
    "    self.image_generator = image_generator\n",
    "    #curr_state will be a four tuple -color, style, mood, composition\n",
    "    self.curr_state = (0,) * len(obs_space)\n",
    "    self.obs_space = obs_space\n",
    "    self.model = model\n",
    "    self.step_count = 0\n",
    "    self.max_iters = 10\n",
    "    if model == 'dqn' or model == 'policy-gradient':\n",
    "      self.rating_history = []\n",
    "      self.action_history = []\n",
    "\n",
    "  def reset(self):\n",
    "    self.curr_state = (0,) * len(self.obs_space)\n",
    "    self.step_count = 0\n",
    "    info = {}\n",
    "    if self.model == 'q-learn':\n",
    "      return self.curr_state\n",
    "\n",
    "    if self.model == 'dqn' or self.model == 'policy-gradient':\n",
    "      self.rating_history = []\n",
    "      self.action_history = []\n",
    "\n",
    "      if self.model == 'policy-gradient':\n",
    "        state = self.curr_state\n",
    "      else:\n",
    "        state = self.state_dqn()\n",
    "\n",
    "      return state, info\n",
    "\n",
    "  def state_dqn(self):\n",
    "    state = np.zeros(128)\n",
    "    info = {}\n",
    "    dims = ['color', 'style', 'mood', 'composition']\n",
    "    features = []\n",
    "    for dim in range(len(self.curr_state)):\n",
    "      features.append(self.curr_state[dim]/len(self.obs_space[dims[dim]]))\n",
    "\n",
    "    if len(self.rating_history) > 0:\n",
    "      features.append(np.mean(self.rating_history[-5:]))\n",
    "      features.append(np.max(self.rating_history))\n",
    "      features.append(np.min(self.rating_history))\n",
    "      features.append(self.rating_history[-1])\n",
    "    else:\n",
    "      features.extend([0,0,0,0])\n",
    "\n",
    "\n",
    "\n",
    "    colors = np.zeros(len(self.obs_space['color']))\n",
    "    styles = np.zeros(len(self.obs_space['style']))\n",
    "    moods = np.zeros(len(self.obs_space['mood']))\n",
    "    compositions = np.zeros(len(self.obs_space['composition']))\n",
    "\n",
    "\n",
    "    for action in self.action_history[-10:]:\n",
    "      colors[action['color']] += 1\n",
    "      styles[action['style']] += 1\n",
    "      moods[action['mood']] += 1\n",
    "      compositions[action['composition']] += 1\n",
    "\n",
    "    if len(self.action_history) > 0:\n",
    "      colors = colors/len(self.action_history[-10:])\n",
    "      styles = styles/len(self.action_history[-10:])\n",
    "      moods = moods/len(self.action_history[-10:])\n",
    "      compositions = compositions/len(self.action_history[-10:])\n",
    "\n",
    "    features.extend(colors[:5])\n",
    "    features.extend(styles[:5])\n",
    "    features.extend(moods[:5])\n",
    "    features.extend(compositions[:5])\n",
    "\n",
    "    features = np.array(features, dtype = np.float32)\n",
    "\n",
    "    features = np.pad(features, 128 - len(features)) if len(features) < 128 else features[:128]\n",
    "\n",
    "    return features\n",
    "\n",
    "  def apply_action(self, action):\n",
    "    # {color: some index, style: some index, mood: some index, composition: some index}\n",
    "\n",
    "    color = self.obs_space['color'][action[0]]\n",
    "    style = self.obs_space['style'][action[1]]\n",
    "    mood = self.obs_space['mood'][action[2]]\n",
    "    composition = self.obs_space['composition'][action[3]]\n",
    "\n",
    "\n",
    "    new_prompt = f\"a beautiful landscape, {color} color palette, {style} style, {mood} mood, {composition} composition\"\n",
    "    return new_prompt\n",
    "\n",
    "  #again action is a four tuple - color, style, mood, composition\n",
    "  def step(self, action, agent):\n",
    "    self.step_count += 1\n",
    "\n",
    "    if self.model == 'policy-gradient': # policy gradient agents directly returns to next state tuple\n",
    "      next_state = action\n",
    "\n",
    "    elif self.model == 'dqn':\n",
    "      self.curr_state = (\n",
    "        action['color'],\n",
    "        action['style'],\n",
    "        action['mood'],\n",
    "        action['composition']\n",
    "        )\n",
    "      next_state = self.curr_state\n",
    "    # existing Q-learn logic\n",
    "    else:\n",
    "      next_state = agent.apply_action(self.curr_state, action)\n",
    "\n",
    "\n",
    "    prompt = self.apply_action(next_state)\n",
    "    image = self.generate_image(prompt)\n",
    "    reward = self.get_rating(image, prompt)\n",
    "    if self.model == 'dqn':\n",
    "      self.rating_history.append(reward)\n",
    "      self.action_history.append(action)\n",
    "\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    if self.step_count >= self.max_iters:\n",
    "      truncated = True\n",
    "\n",
    "    # termination logic for dqn\n",
    "    if self.model == 'dqn' and len(self.action_history) > 3:\n",
    "      if all(r>.5 for r in self.rating_history[-3:]):\n",
    "        terminated = True\n",
    "\n",
    "    self.curr_state = next_state\n",
    "    info = {'image':image, 'prompt':prompt}\n",
    "\n",
    "    if self.model == 'q-learn':\n",
    "      return next_state, reward, info\n",
    "    if self.model == 'policy-gradient':\n",
    "      return next_state, reward, info, terminated, truncated\n",
    "    # existing dqn logic\n",
    "    else:\n",
    "      state = self.state_dqn()\n",
    "      return state, reward, info, terminated, truncated\n",
    "\n",
    "  #simply have user prompt and the agent will slowly learn the artist's style\n",
    "  #num of inferference steps set low for efficiency but real implementation should\n",
    "  #use advanced ai image generators anyways so this is for training purposes!\n",
    "  def generate_image(self, prompt):\n",
    "    image = self.image_generator(\n",
    "        prompt,\n",
    "        num_inference_steps=20\n",
    "    ).images[0]\n",
    "    return image\n",
    "\n",
    "  #the reward is from the artist's rating with 5 = higher and 1 = low reward\n",
    "  def get_rating(self, image, prompt):\n",
    "    display(image)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Rate this image:\")\n",
    "    rating = int(input(\"Rate 1-5:\"))\n",
    "    #squish the values between -1 and 1 so range is like 5:1 4:.5 3:0 2:-.5 1:-1 so 1 penalizes while 5 gives greatest reward\n",
    "    return (rating - 3)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanInLoopAgent():\n",
    "  def __init__(self, learning_rate, initial_epsilon, epsilon_decay, final_epsilon, discount_factor, q_table = False):\n",
    "    self.lr = learning_rate\n",
    "    self.epsilon = initial_epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.final_epsilon = final_epsilon\n",
    "    self.discount_factor = discount_factor\n",
    "    #Q-table will be Q[state][action] = value (float reward)\n",
    "    self.q_table = q_table if q_table else defaultdict(lambda: defaultdict(float))\n",
    "    self.dim_space = 5\n",
    "    self.dims = ['color', 'style', 'mood', 'composition']\n",
    "\n",
    "  def action_space(self):\n",
    "    actions = []\n",
    "    for dim in self.dims:\n",
    "      for val in range(self.dim_space):\n",
    "        actions.append((dim, val))\n",
    "    return actions\n",
    "\n",
    "  #state here is a four tuple (color, style, mood, comp) and now choose either opt or explore new from all actions\n",
    "  def choose_action(self, state):\n",
    "    #either explore or exploit\n",
    "    if np.random.random() < self.epsilon:\n",
    "      dim = random.choice(self.dims)\n",
    "      val = random.randint(0,self.dim_space-1)\n",
    "      action = (dim, val)\n",
    "    else:\n",
    "      actions = self.action_space()\n",
    "      q_values = [self.q_table[state][action] for action in actions]\n",
    "      max_q = max(q_values)\n",
    "      best_actions = []\n",
    "      for action, q in zip(actions, q_values):\n",
    "        if q == max_q:\n",
    "          best_actions.append(action)\n",
    "      action = random.choice(best_actions)\n",
    "    return action\n",
    "\n",
    "  def decay_epsilon(self):\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "  def update(self, state, action, reward, next_state):\n",
    "    '''\n",
    "    Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]\n",
    "    '''\n",
    "    current_q = self.q_table[state][action]\n",
    "    actions = self.action_space()\n",
    "    q_values = [self.q_table[next_state][action] for action in actions]\n",
    "    max_next_q = max(q_values) if q_values else 0\n",
    "    new_q = current_q + self.lr * (reward + self.discount_factor * max_next_q - current_q)\n",
    "    self.q_table[state][action] = new_q\n",
    "\n",
    "  def apply_action(self, curr_state, best_action):\n",
    "    #here best_action is like ('color',3)\n",
    "    #curr_state = (2,3,3,3)\n",
    "    dim, val = best_action\n",
    "    new_state = list(curr_state)\n",
    "    new_state[self.dims.index(dim)] = val\n",
    "    return tuple(new_state)\n",
    "\n",
    "  def get_best_action(self, curr_state):\n",
    "    actions = self.action_space()\n",
    "    q_values = [self.q_table[curr_state][action] for action in actions]\n",
    "    best_action = actions[np.argmax(q_values)]\n",
    "    return self.apply_action(curr_state, best_action)\n",
    "\n",
    "  def save_style(self, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "      data = {\n",
    "          'q_table':dict(self.q_table),\n",
    "          'lr':self.lr,\n",
    "          'epsilon':self.epsilon,\n",
    "          'epsilon_decay':self.epsilon_decay,\n",
    "          'final_epsilon':self.final_epsilon,\n",
    "          'discount_factor': self.discount_factor\n",
    "      }\n",
    "      pickle.dump(data, file)\n",
    "\n",
    "  def load_style(self, filename):\n",
    "    if not os.path.exists(filename):\n",
    "      return None\n",
    "    with open(filename, 'rb') as file:\n",
    "      data = pickle.load(file)\n",
    "      q_table = defaultdict(lambda: defaultdict(float), data['q_table'])\n",
    "      agent = HumanInLoopAgent(data['lr'], data['epsilon'], data['epsilon_decay'], data['final_epsilon'], data['discount_factor'], q_table)\n",
    "      return agent\n",
    "\n",
    "  def generate_my_image(prompt, env):\n",
    "    best_state = agent.get_best_action(env.curr_state)\n",
    "    print(f\"Best discovered settings: {best_state}\")\n",
    "    prompt = env.apply_action(best_state, testing = True)\n",
    "    print(f\"\\nGenerating image with: {prompt}\")\n",
    "    final_image = env.generate_image(prompt)\n",
    "    display(final_image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_Glm26XfTBi"
   },
   "source": [
    "Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.2\n",
    "start_epsilon = .8\n",
    "epsilon_decay = .88\n",
    "final_epsilon = 0.1\n",
    "discount_factor = .95\n",
    "n_iters = 20\n",
    "env = ArtCreationEnv(pipe, {\n",
    "            'color': ['warm', 'cool', 'vibrant', 'muted', 'monochrome'],\n",
    "            'style': ['realistic', 'painterly', 'abstract', 'minimalist', 'surreal'],\n",
    "            'mood': ['peaceful', 'energetic', 'mysterious', 'joyful', 'melancholic'],\n",
    "            'composition': ['centered', 'rule-of-thirds', 'asymmetric', 'symmetrical', 'dynamic']\n",
    "        })\n",
    "\n",
    "agent = HumanInLoopAgent(learning_rate, start_epsilon, epsilon_decay, final_epsilon, discount_factor)\n",
    "\n",
    "for i in tqdm(range(n_iters)):\n",
    "  state = env.curr_state if i > 0 else env.reset()\n",
    "  action = agent.choose_action(state)\n",
    "  next_state, reward,info = env.step(action, agent)\n",
    "\n",
    "  agent.update(state, action, reward, next_state)\n",
    "  agent.decay_epsilon()\n",
    "\n",
    "  print(f\"Reward: {reward:.2f}\")\n",
    "  print(f\"Current best settings: {agent.get_best_action(next_state)}\")\n",
    "\n",
    "best_state = agent.get_best_action(env.curr_state)\n",
    "print(f\"Best discovered settings: {best_state}\")\n",
    "\n",
    "# Generate final image with best settings\n",
    "final_prompt = env.apply_action(best_state)\n",
    "print(f\"\\nGenerating final image with: {final_prompt}\")\n",
    "final_image = env.generate_image(final_prompt)\n",
    "display(final_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrnxk5er-qHb"
   },
   "source": [
    "Save the model and Load and Test It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learned now can use\n",
    "best_state = agent.get_best_action(env.curr_state)\n",
    "print(f\"Best discovered settings: {best_state}\")\n",
    "prompt = env.apply_action(best_state)\n",
    "print(f\"\\nGenerating image with: {prompt}\")\n",
    "final_image = env.generate_image(prompt)\n",
    "display(final_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQmii495ZADK"
   },
   "source": [
    "Our Q-table is way too big! 5 * 5 * 5* 5 = 625 States and each state has 20 different possible actions = 12,500 entries. Let's pivot to Deep Q-learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoulCxfNlAxQ"
   },
   "source": [
    "Now, expand the artist choices(feedback) and because we're working with a high dimensional space now let's pivot to using a new approach of policy gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q75mzoEcpnr0"
   },
   "source": [
    "###Let's expand the states space and for this. We'll implement Deep Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ate_ttbGsc-f"
   },
   "source": [
    "Sources used: https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import math\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replay memory -> decorrelates experiences by randomly sampling which improves generalization\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "#named tuple is part of python library and allows me to access Transition via Transition.state, Transition.action, Transition.next_state, etc\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a neural network - where it chooses from each category hence four separate action spaces\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations):\n",
    "        super().__init__()\n",
    "\n",
    "        self.start = nn.Sequential(\n",
    "            nn.Linear(n_observations, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.color = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 22)\n",
    "        )\n",
    "\n",
    "        self.style = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 18)\n",
    "        )\n",
    "\n",
    "        self.mood = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16)\n",
    "        )\n",
    "\n",
    "        self.composition = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 15)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = self.start(x)\n",
    "        return {\n",
    "            'color': self.color(start),\n",
    "            'style': self.style(start),\n",
    "            'mood': self.mood(start),\n",
    "            'composition': self.composition(start)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 2500\n",
    "#note how the update rate is small this is to ensure it is as stable as possible\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "  global steps_done\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "  steps_done += 1\n",
    "  if sample > eps_threshold:\n",
    "      #exploit by taking max Q-value for each of the 4 categories\n",
    "    with torch.no_grad():\n",
    "      q_values = policy_net(state)\n",
    "      action = {\n",
    "            'color': torch.argmax(q_values['color']).item(),\n",
    "            'style': torch.argmax(q_values['style']).item(),\n",
    "            'mood': torch.argmax(q_values['mood']).item(),\n",
    "            'composition': torch.argmax(q_values['composition']).item()\n",
    "          }\n",
    "  else:\n",
    "    #explore\n",
    "    action = {\n",
    "                'color': random.randint(0,21),\n",
    "                'style': random.randint(0,17),\n",
    "                'mood': random.randint(0,15),\n",
    "                'composition': random.randint(0,14)\n",
    "            }\n",
    "  return action\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    #Now can call\n",
    "    #batch.state\n",
    "    #batch.action\n",
    "    #batch.next_state\n",
    "\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #if next_state is None turn to False\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    #unsqueeze adds another dimension for the next step when we perform .gather operation\n",
    "    color_batch = torch.tensor([action['color'] for action in batch.action], device = device).unsqueeze(1)\n",
    "    style_batch = torch.tensor([action['style'] for action in batch.action], device = device).unsqueeze(1)\n",
    "    mood_batch = torch.tensor([action['mood'] for action in batch.action], device = device).unsqueeze(1)\n",
    "    composition_batch = torch.tensor([action['composition'] for action in batch.action],device = device).unsqueeze(1)\n",
    "\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    #Get q_values\n",
    "    q_values = policy_net(state_batch) #.gather(1, action_batch)\n",
    "    color_q_values = q_values['color'].gather(1, color_batch)\n",
    "    style_q_values = q_values['style'].gather(1, style_batch)\n",
    "    mood_q_values = q_values['mood'].gather(1, mood_batch)\n",
    "    composition_q_values = q_values['composition'].gather(1, composition_batch)\n",
    "    state_action_values = (color_q_values + style_q_values + mood_q_values + composition_q_values)/4\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    #get next state q values from target net\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(non_final_next_states)\n",
    "        next_color = next_q['color'].max(1).values\n",
    "        next_style = next_q['style'].max(1).values\n",
    "        next_mood = next_q['mood'].max(1).values\n",
    "        next_composition = next_q['composition'].max(1).values\n",
    "        next_state_values[non_final_mask] = (next_color + next_style + next_mood + next_composition)/4\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Training\n",
    "#modified observation space\n",
    "obs_space = {\n",
    "            'color': ['warm', 'cool', 'neutral', 'dark', 'light', 'vibrant', 'muted', 'monochrome', 'analogous', 'complementary', 'pastel', 'earth-tones','neon','modern','bohemian','spring','summer','winter','autumn','gradient','impressionist','expressionist'],\n",
    "            'style': [\"photorealistic\", \"hyperrealistic\", \"realistic\",\"impressionist\", \"expressionist\", \"abstract_expressionist\",\"minimalist\", \"geometric\", \"abstract\",\"surreal\", \"dreamlike\", \"ethereal\",\"oil_painting\", \"watercolor\", \"pencil_sketch\",\n",
    "                      \"digital_art\", \"concept_art\", \"3d_render\"],\n",
    "            'mood': [\"calm\", \"peaceful\", \"energetic\", \"chaotic\",\"joyful\", \"melancholic\", \"mysterious\", \"ominous\",\"bright\", \"dark\", \"moody\", \"ethereal\", \"dramatic\",\"warm\", \"cool\", \"neutral\"],\n",
    "            'composition': [\"centered\", \"symmetrical\", \"asymmetrical\",\"rule_of_thirds\", \"golden_ratio\", \"diagonal\",\"shallow_depth\", \"deep_depth\", \"layered\",\"close_up\", \"wide_shot\", \"portrait\", \"landscape\",\"static\", \"dynamic\", \"flowing\"]\n",
    "}\n",
    "env = ArtCreationEnv(pipe, obs_space, model = 'dqn')\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations).to(device)\n",
    "target_net = DQN(n_observations).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "n_iters = 20\n",
    "steps_done = 0\n",
    "\n",
    "for i_episode in tqdm(range(n_iters)):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    print(state)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    episode_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        print(\"here\",action)\n",
    "        observation, reward, info, terminated, truncated = env.step(action, None)\n",
    "        reward_value = reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward_value\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {i_episode+1} finished after {t+1} steps, Total Reward: {episode_reward:.2f}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taNS2MvZlLVS"
   },
   "source": [
    "###Policy Gradient\n",
    "Input: state (4-dims or 20-d one-hot)\n",
    "-->\n",
    "Shared hidden layers\n",
    "-->\n",
    "[color head (5 logits), style head (5 logits), mood head (5 logits), composition head (5 logits)]\n",
    "*   each head --> softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmgrJhdrlg7D"
   },
   "source": [
    "Import Necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolicyNetwork -- the Neural Network Architecture\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=4):\n",
    "      super(PolicyNetwork, self).__init__()\n",
    "\n",
    "      # shared feature extractor\n",
    "      self.shared = nn.Sequential(\n",
    "          nn.Linear(input_dim, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 64),\n",
    "          nn.ReLU()\n",
    "      )\n",
    "\n",
    "      # separate output heads for each output dimension\n",
    "      self.color_head = nn.Linear(64, 5)\n",
    "      self.style_head = nn.Linear(64, 5)\n",
    "      self.mood_head = nn.Linear(64, 5)\n",
    "      self.comp_head = nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, state):\n",
    "      # state: tensor of shape (4,)\n",
    "      # returns: list of logits (not probabilities yet)\n",
    "\n",
    "      # convert state tuple/list to Float Tensor if needed\n",
    "      if not isinstance(state, torch.Tensor):\n",
    "        x = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(next(self.parameters()).device)\n",
    "      else:\n",
    "        x = state\n",
    "\n",
    "      shared_features = self.shared(x)\n",
    "\n",
    "      return {\"color\": self.color_head(shared_features),\n",
    "              \"style\": self.style_head(shared_features),\n",
    "              \"mood\": self.mood_head(shared_features),\n",
    "              \"composition\": self.comp_head(shared_features)}\n",
    "\n",
    "    def select_action(self, state):\n",
    "      # get logits for the 4 dimensions\n",
    "      logits_dict = self.forward(state)\n",
    "\n",
    "      log_prob_sum = torch.tensor(0.0).to(next(self.parameters()).device)\n",
    "      new_state_list = []\n",
    "\n",
    "      for dim, logits in logits_dict.items():\n",
    "        # create categorical distribution for each logit\n",
    "        dist = Categorical(logits=logits) # softmax is implicit\n",
    "        # sample an index (0 - 4)\n",
    "        action = dist.sample()\n",
    "\n",
    "        log_prob = dist.log_prob(action).squeeze()\n",
    "        log_prob_sum += log_prob\n",
    "        new_state_list.append(action.item())\n",
    "\n",
    "      # we want to return a 4-d new state (a tuple of 4 indices)\n",
    "      new_state = tuple(new_state_list)\n",
    "\n",
    "      # log_prob_sum is a tensor; squeeze is used to get a single value\n",
    "      return new_state, log_prob_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient Agent Class\n",
    "class PolicyGradientAgent():\n",
    "  def __init__(self, learning_rate, discount_factor):\n",
    "    self.policy_net = PolicyNetwork(input_dim=4).to(device) # will use PolicyNetwork defined above\n",
    "    self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "    self.discount_factor = discount_factor\n",
    "    self.episode_data = [] # stores (log_prob, reward) for one episode\n",
    "\n",
    "  def choose_action(self, state):\n",
    "    new_state, log_prob = self.policy_net.select_action(state)\n",
    "    self.episode_data.append((log_prob, None)) # store the log_prob and no reward for now\n",
    "    return new_state\n",
    "\n",
    "  # adds the reward to the most recent entry in episode buffer\n",
    "  def store_reward(self, reward):\n",
    "    if self.episode_data:\n",
    "      last_entry = self.episode_data[-1]\n",
    "      self.episode_data[-1] = (last_entry[0], reward)\n",
    "\n",
    "  def get_best_state(self, curr_state):\n",
    "    # convert state to tensor if needed\n",
    "    if not isinstance(curr_state, torch.Tensor):\n",
    "          curr_state = torch.tensor(curr_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits_dict = self.policy_net.forward(curr_state)\n",
    "\n",
    "      # for each head, find index with maximum logit (highest probability)\n",
    "      best_color_idx = torch.argmax(logits_dict['color']).item()\n",
    "      best_style_idx = torch.argmax(logits_dict['style']).item()\n",
    "      best_mood_idx = torch.argmax(logits_dict['mood']).item()\n",
    "      best_comp_idx = torch.argmax(logits_dict['composition']).item()\n",
    "\n",
    "      return (best_color_idx, best_style_idx, best_mood_idx, best_comp_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforce update function\n",
    "def update_policy(agent):\n",
    "  # Loss = - (log_prob * discounted_return)\n",
    "\n",
    "  if not agent.episode_data:\n",
    "    return\n",
    "\n",
    "  # prepare data\n",
    "  log_probs = [data[0] for data in agent.episode_data]\n",
    "  rewards = [data[1] for data in agent.episode_data]\n",
    "\n",
    "  # calculate discounted returns (G_t)\n",
    "  R = 0\n",
    "  returns = []\n",
    "\n",
    "  for r in rewards[::-1]: # iterate through episode backwards\n",
    "    R = r + agent.discount_factor * R\n",
    "    returns.insert(0, R)\n",
    "\n",
    "  returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "  returns = (returns - returns.mean()) / (returns.std() + 1e-9) # normalize returns\n",
    "\n",
    "  # calculate the policy loss\n",
    "  policy_loss = []\n",
    "  for log_prob, R in zip(log_probs, returns):\n",
    "    policy_loss.append(-log_prob * R)\n",
    "\n",
    "  # optimization\n",
    "  agent.optimizer.zero_grad()\n",
    "  policy_loss = torch.stack(policy_loss).sum()  # summing losses from the episode\n",
    "  policy_loss.backward()\n",
    "  agent.optimizer.step()\n",
    "\n",
    "  agent.episode_data = [] # reset episode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Policy Gradient Agent\n",
    "\n",
    "# set up hyperparameters\n",
    "LEARNING_RATE = 5e-4\n",
    "GAMMA = 0.99\n",
    "NUM_EPISODES = 5\n",
    "MAX_ITERATIONS_PER_EPISODE = 20\n",
    "#modified observation space\n",
    "obs_space_pg = {\n",
    "    'color': ['warm', 'cool', 'vibrant', 'muted', 'monochrome'],\n",
    "    'style': ['realistic', 'painterly', 'abstract', 'minimalist', 'surreal'],\n",
    "    'mood': ['peaceful', 'energetic', 'mysterious', 'joyful', 'melancholic'],\n",
    "    'composition': ['centered', 'rule-of-thirds', 'asymmetric', 'symmetrical', 'dynamic']\n",
    "}\n",
    "env = ArtCreationEnv(pipe, obs_space_pg, model='policy-gradient')\n",
    "agent = PolicyGradientAgent(learning_rate=LEARNING_RATE, discount_factor=GAMMA)\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "print(f\"Starting Policy Gradient (REINFORCE) Training for {NUM_EPISODES} episodes...\")\n",
    "\n",
    "for i_episode in tqdm(range(NUM_EPISODES)):\n",
    "  state, info = env.reset()\n",
    "  Done = False\n",
    "  episode_reward_total = 0\n",
    "\n",
    "  for t in range(MAX_ITERATIONS_PER_EPISODE):\n",
    "    new_state = agent.choose_action(state)\n",
    "    next_state, reward, info, terminated, truncated = env.step(new_state, agent)\n",
    "    agent.store_reward(reward)  # store reward the user provided\n",
    "\n",
    "    # updating the state\n",
    "    state = next_state\n",
    "    episode_reward_total += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "      break\n",
    "\n",
    "  # episode end -- perform policy update\n",
    "  update_policy(agent)\n",
    "\n",
    "  episode_rewards.append(episode_reward_total)\n",
    "  print(f\"Episode {i_episode+1} finished. Total Reward: {episode_reward_total:.2f}. Last State: {state}\")\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "final_state = agent.get_best_state(env.curr_state)\n",
    "print(f\"Best learned state indices: {final_state}\")\n",
    "final_prompt = env.apply_action(final_state)\n",
    "\n",
    "# generating image with best settings\n",
    "print(f\"\\nGenerating final image with: {final_prompt}\")\n",
    "final_image = env.generate_image(final_prompt)\n",
    "display(final_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv (Python 3.11)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
